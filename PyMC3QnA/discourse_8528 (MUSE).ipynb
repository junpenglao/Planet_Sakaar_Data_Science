{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0827192-9773-4bf9-a334-21cb9844246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "import aesara\n",
    "import aesara.tensor as at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8535ab42-3b81-4cb9-9383-a4884f6cf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_EXAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a3ad92-8319-49b7-998b-78ffe456bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(SIMPLE_EXAMPLE, x_obs=None):\n",
    "    if SIMPLE_EXAMPLE:\n",
    "        with pm.Model() as funnel:\n",
    "            θ = pm.Normal(\"θ\", 0, 3)\n",
    "            z = pm.Normal(\"z\", 0, at.exp(θ / 2), size=512)\n",
    "            x = pm.Normal(\"x\", z, 1, observed=x_obs)\n",
    "    else:\n",
    "        with pm.Model() as funnel:\n",
    "            θ0 = pm.Normal(\"θ0\", 0, 3)\n",
    "            z0 = pm.Normal(\"z0\", 0, at.exp(θ0 / 2), size=256)\n",
    "            θ1 = pm.HalfNormal(\"θ1\", 5)\n",
    "            z1 = pm.Normal(\"z1\", 0, θ1, size=(128, 2))\n",
    "            σ = pm.HalfNormal(\"σ\", 1)\n",
    "            x = pm.Normal(\"x\", at.stack([z0, at.flatten(z1)]), σ, observed=x_obs)\n",
    "    return funnel\n",
    "\n",
    "m = gen_model(SIMPLE_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b4b49-7ac1-4b8d-a9a4-366d7a757286",
   "metadata": {},
   "source": [
    "Generate data for `x` condition on some true $\\theta$. There are a few ways to do it as explained in https://github.com/pymc-devs/pymc/discussions/5280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b9d481-9686-40cf-a79e-89f553f9fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMPLE_EXAMPLE:\n",
    "    sample_x: callable = aesara.function([m.θ], [m.x])\n",
    "    x_obs, = sample_x(1.)\n",
    "else:\n",
    "    sample_x: callable = aesara.function([m.θ0, m.θ1, m.σ], [m.x])\n",
    "    x_obs, = sample_x(0., 1., 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6888bd47-1f2f-425c-a624-cb39f3542b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c3db7-b98e-44ab-86aa-2ddaf2d5953b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a regular PyMC model that conditioned on some observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30396006-2eef-4a07-bb68-c9afe874dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel = gen_model(SIMPLE_EXAMPLE, x_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cba126-7925-4fc4-9233-fed52a40db46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Forward sampling function (for generating `x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d66aff-ab8d-4d2a-80a9-6fd700b22b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x_z = aesara.function([θ], [z, x])\n",
    "model_graph = pm.model_graph.ModelGraph(funnel)\n",
    "# theta are priors with no parent\n",
    "theta = [var for var in funnel.basic_RVs if model_graph.get_parents(var) == set()]\n",
    "# The remaining free variables are z\n",
    "latent_field = [var for var in funnel.free_RVs if var not in theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16668ae9-2780-4056-a2c9-60a2fd2180ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[θ0, z0, θ1, z1, σ, x]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funnel.basic_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba28c640-df3b-46e2-8380-642c9909c50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([θ0, θ1, σ], [z0, z1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta, latent_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522d51a7-fb65-42da-bc2f-160baa6aa89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 256), (256,), (128, 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z_x = list(set(funnel.value_vars) - set(theta))  # Not doing this as the order is unpredictable\n",
    "z_x: list = funnel.observed_RVs + latent_field\n",
    "\n",
    "sample_x_z: callable = aesara.function(theta, z_x)\n",
    "theta_val = [v.eval() for v in theta]\n",
    "output_test = sample_x_z(*theta_val)\n",
    "[v.shape for v in output_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f27cbd-4b01-4aab-a710-d0a0d1afa578",
   "metadata": {},
   "source": [
    "Alternative `sample_x` that only output the simulation of the observed (`x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9015bf32-0266-48c1-a8fe-defd7f53a61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 256)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x: callable = aesara.function(theta, funnel.observed_RVs)\n",
    "theta_val = [v.eval() for v in theta]\n",
    "output_test = sample_x(*theta_val)\n",
    "[v.shape for v in output_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317540b-37e9-494c-90d1-2d019a23b2a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Likelihood function `logP(x,z|θ)` and Prior function `logP(θ)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ba9032-9b44-4a5d-b8ee-53a636af06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc.distributions import logpt as joint_logpt\n",
    "\n",
    "# Copy and small modification of self.logp_elemwiset in a pm.Model\n",
    "def generate_logpt_allnodes(model, vars: list, jacobian: bool = True):\n",
    "    \"\"\"Elemwise log-probability of the input variables.\"\"\"\n",
    "    if model.potentials:\n",
    "        raise Exception(\"Does not work with model that contains potentials\")\n",
    "\n",
    "    rv_values = {}\n",
    "    for var in vars:\n",
    "        if var in model.observed_RVs:\n",
    "            value_var = var.type()\n",
    "            value_var.name = var.name\n",
    "        else:\n",
    "            value_var = model.rvs_to_values[var]\n",
    "        if value_var is not None:\n",
    "            rv_values[var] = value_var\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Requested variable {var} not found among the model variables\"\n",
    "            )\n",
    "\n",
    "    rv_logps = joint_logpt(list(rv_values.keys()), rv_values, sum=False, jacobian=jacobian)\n",
    "    logpt_nodes = {}\n",
    "    for k, logp in zip(rv_values.keys(), rv_logps):\n",
    "        node_logp = logp.sum()\n",
    "        node_logp.name = k.name + \"_logpt\"\n",
    "        logpt_nodes[k] = node_logp\n",
    "\n",
    "    return logpt_nodes, rv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b668e5c-e3c3-49c1-9682-0bec34a3ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the order of z, x, θ\n",
    "ordered_input_var: list = latent_field + funnel.observed_RVs + theta\n",
    "\n",
    "logpt_nodes, rv_values = generate_logpt_allnodes(funnel, ordered_input_var)\n",
    "input_var = [rv_values[v] for v in ordered_input_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4cf0351-24bb-4536-b142-4572fe79bd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{z0: z0_logpt,\n",
       " z1: z1_logpt,\n",
       " x: x_logpt,\n",
       " θ0: θ0_logpt,\n",
       " θ1: θ1_logpt,\n",
       " σ: σ_logpt}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logpt_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c7bbe8a-995f-4dd1-ba30-c01d0fba772b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{z0: z0, z1: z1, x: x, θ0: θ0, θ1: θ1_log__, σ: σ_log__}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1536bf2f-1014-46df-965d-bb4d86fcf124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-235.2482645), array(-647.26437008), array(-1710.18395397), array(-2.01755082), array(-0.72579135), array(-0.72579135)]\n",
      "[z0, z1, x, θ0, θ1, σ]\n",
      "\n",
      "From PyMC model itself\n",
      "θ0      -2.02\n",
      "z0    -235.25\n",
      "θ1      -0.73\n",
      "z1    -647.26\n",
      "σ       -0.73\n",
      "x    -1710.18\n",
      "Name: Point log-probability, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Some testing\n",
    "compile_logp_fn_per_node = aesara.function(\n",
    "    input_var, [logpt_nodes[v] for v in ordered_input_var])\n",
    "test_point = funnel.recompute_initial_point()\n",
    "\n",
    "z_val = [test_point[rv_values[v].name] for v in latent_field]\n",
    "θ_val = [test_point[rv_values[v].name] for v in theta]\n",
    "\n",
    "x_val = [funnel.rvs_to_values[v].data for v in funnel.observed_RVs]\n",
    "\n",
    "print(compile_logp_fn_per_node(*z_val, *x_val, *θ_val))\n",
    "print(ordered_input_var)\n",
    "print(\"\\nFrom PyMC model itself\")\n",
    "print(funnel.point_logps(test_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8669dade-0184-4edd-b9df-7cd8237e4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpt_z_x = []\n",
    "logpt_theta = []\n",
    "input_theta = []\n",
    "for var in ordered_input_var:\n",
    "    if var not in theta:\n",
    "        logpt_z_x.append(logpt_nodes[var])\n",
    "    else:\n",
    "        logpt_theta.append(logpt_nodes[var])\n",
    "        input_theta.append(rv_values[var])\n",
    "\n",
    "condition_logpt = at.sum(logpt_z_x)\n",
    "compile_logp_fn = aesara.function(input_var, [condition_logpt])\n",
    "theta_logpt = at.sum(logpt_theta)\n",
    "compile_logp_fn_theta = aesara.function(input_theta, [theta_logpt])\n",
    "\n",
    "# compare to full posterior as check\n",
    "np.testing.assert_almost_equal(sum(compile_logp_fn(*z_val, *x_val, *θ_val) + compile_logp_fn_theta(*θ_val)),\n",
    "                               funnel.logp(test_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddf65c-53d3-4b38-b3ca-30c35d4d47a6",
   "metadata": {},
   "source": [
    "## ∇θ_logLike and ∇θ_logPrior(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69470930-b7ca-4aa4-9be5-a85e2aeaa381",
   "metadata": {},
   "source": [
    "∇θ_logLike is gradient of θ -> logP(x,z|θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dab433e-086f-4db0-a98b-13776bb7adbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(-128.), array(-256.), array(1967.37484993)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_theta_logpt = aesara.grad(\n",
    "    condition_logpt, \n",
    "    wrt=input_theta, \n",
    "    consider_constant=list(set(input_var) - set(input_theta))\n",
    ")\n",
    "compile_grad_theta_fn = aesara.function(input_var, grad_theta_logpt)\n",
    "compile_grad_theta_fn(*z_val, *x_val, *θ_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac4d76-cee5-4d67-8a74-571b8f2879c2",
   "metadata": {},
   "source": [
    "∇θ_logPrior(θ) is gradient of θ -> logPrior(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c02af32b-0b13-467e-a5a9-80614d57058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(-0.), array(1.11022302e-16), array(0.)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_theta_prior = aesara.grad(\n",
    "    theta_logpt, \n",
    "    wrt=input_theta\n",
    ")\n",
    "compile_grad_theta_fn = aesara.function(input_theta, grad_theta_prior)\n",
    "compile_grad_theta_fn(*θ_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92b965-b968-4140-b553-66bf0fbe1120",
   "metadata": {
    "tags": []
   },
   "source": [
    "## zMAP that maximizes the function z -> logP(x,z|θ)\n",
    "\n",
    "We can use [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) to find the MAP.\n",
    "While we have logP(x,z|θ) and gradient of z -> logP(x,z|θ) above, to make using scipy minimize easier we to some additional formatting so that:\n",
    "- `cost_fun` to return a tuple (f, g) containing the objective function and the gradient\n",
    "- Flatten `z` to an 1-D array with shape (n,)\n",
    "\n",
    "Similar logic and API useage see [find_MAP in PyMC](https://github.com/pymc-devs/pymc/blob/1d7130d8cf6e419120e192f8308cf154f4c44074/pymc/tuning/starting.py#L148-L150) and [ValueGradFunction](https://github.com/pymc-devs/pymc/blob/5626a04a1e064ad615e1765e37bcb7ea52887ab7/pymc/model.py#L358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a4e2255-c626-4aec-93fc-596b729df57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ∇z_logLike is gradient of z -> logP(x,z|θ) that will be used in optimization\n",
    "input_z = []\n",
    "input_x_theta = []\n",
    "for var in ordered_input_var:\n",
    "    if var in latent_field:\n",
    "        input_z.append(rv_values[var])\n",
    "    else:\n",
    "        input_x_theta.append(rv_values[var])\n",
    "\n",
    "# Flatten and replace value (similar to ValueGradFunction in pm.Model)\n",
    "flatten_z = at.vector(name='flatten_z')\n",
    "split_point = np.concatenate([np.asarray([0]), np.cumsum([v.size for v in z_val])], axis=-1)\n",
    "z_replace = []\n",
    "for i, np_val in enumerate(z_val):\n",
    "    z_replace.append(at.reshape(flatten_z[split_point[i]:split_point[i+1]], np_val.shape))\n",
    "\n",
    "mapping_fn = aesara.function([flatten_z], z_replace)\n",
    "flatten_z_val = np.concatenate([v.ravel() for v in z_val], axis=-1)\n",
    "# mapping_fn(flatten_z_val)\n",
    "\n",
    "# We minimize the negative loglikelihood\n",
    "condition_logpt_clone = -1.0 * aesara.clone_replace(condition_logpt, dict(zip(input_z, z_replace)))\n",
    "grad_z_clone_tensor = aesara.grad(\n",
    "    condition_logpt_clone,\n",
    "    wrt=flatten_z, \n",
    "    consider_constant=input_x_theta\n",
    ")\n",
    "cost_fun_with_grad = aesara.function(\n",
    "    [flatten_z] + input_x_theta, \n",
    "    [condition_logpt_clone, grad_z_clone_tensor])\n",
    "value_test, grad_test = cost_fun_with_grad(flatten_z_val, *x_val, *θ_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad92c7e9-27fd-493c-aa0c-a63014d213df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing\n",
    "grad_z_tensor = aesara.grad(\n",
    "    -condition_logpt,\n",
    "    wrt=input_z, \n",
    "    consider_constant=input_x_theta\n",
    ")\n",
    "\n",
    "output_tensors = [-condition_logpt] + grad_z_tensor\n",
    "cost_fun_with_grad_ = aesara.function(input_var, output_tensors)\n",
    "value_test2, *grad_test2 = cost_fun_with_grad_(*z_val, *x_val, *θ_val)\n",
    "\n",
    "assert value_test == value_test2\n",
    "_ = [np.testing.assert_almost_equal(v1, v2) for v1, v2 in zip(mapping_fn(grad_test), grad_test2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d76a0-b0df-45d2-a4d5-5e83fb95419f",
   "metadata": {},
   "source": [
    "### Using PyMC idioms\n",
    "\n",
    "But either have the exact input -> output signature we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45af9f0b-f152-49f7-b47a-896eeebcdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point['x'] = x_val[0]\n",
    "# shared = pm.make_shared_replacements(test_point, input_z, funnel)\n",
    "shared = {\n",
    "    var: aesara.shared(test_point[var.name],\n",
    "                       var.name + \"_shared\",\n",
    "                       broadcastable=var.broadcastable)\n",
    "    for var in input_x_theta\n",
    "}\n",
    "out_list, inarray0 = pm.join_nonshared_inputs(\n",
    "    test_point, [-condition_logpt] + grad_z_tensor, input_var, shared)\n",
    "cost_fun_with_grad_ = pm.aesaraf.compile_pymc([inarray0], out_list)\n",
    "cost_fun_with_grad_.trust_input = True\n",
    "value_test3, *grad_test3 = cost_fun_with_grad_(flatten_z_val)\n",
    "\n",
    "assert value_test2 == value_test3\n",
    "_ = [np.testing.assert_almost_equal(v1, v2) for v1, v2 in zip(grad_test2, grad_test3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "129fc895-ca95-4669-a023-a6bc7e2cb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_vars_and_values = {\n",
    "    var: test_point[var.name]\n",
    "    for var in input_x_theta\n",
    "}\n",
    "pm_val_grad_fn = pm.model.ValueGradFunction([-condition_logpt], input_z, extra_vars_and_values)\n",
    "pm_val_grad_fn.set_extra_values(test_point)\n",
    "value_test4, grad_test4 = pm_val_grad_fn(z_val)\n",
    "\n",
    "assert value_test == value_test3\n",
    "np.testing.assert_almost_equal(grad_test, grad_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915b9ae-1acb-4e86-bb13-407119eeeea3",
   "metadata": {},
   "source": [
    "Once we have the logp fn that also output gradient, optimizing it is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7852857-3b66-44fa-9968-86e287c57afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def zmap_optimization(\n",
    "    cost_fun_with_grad: callable,\n",
    "    initial_z: list,\n",
    "    x: list,\n",
    "    theta: list,\n",
    "    random_init=True,\n",
    "    method='L-BFGS-B',\n",
    "    **kwargs):\n",
    "    x0 = np.concatenate([v.ravel() for v in initial_z], axis=-1)\n",
    "    if random_init:\n",
    "        x0 = np.random.randn(*x0.shape)\n",
    "    return minimize(\n",
    "        cost_fun_with_grad, x0, args=(*x, *theta), method=method, jac=True, **kwargs\n",
    "    )\n",
    "\n",
    "output = zmap_optimization(cost_fun_with_grad, z_val, x_val, θ_val, random_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6ade842-0a17-4013-a878-f596320533ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "zmap_val = mapping_fn(output.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "198b60b2-1151-4e82-b463-4359487c2eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(31.17454171), array(-211.40023662), array(-191.86660666)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compile_grad_theta_fn = aesara.function(input_var, grad_theta_logpt)\n",
    "compile_grad_theta_fn(*zmap_val, *x_val, *θ_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3fdc3b-8036-4c22-b9ea-5e74ec64a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-95.47144803), array(-231.80552749), array(-445.97514265)]\n",
      "[array(-96.52885436), array(-230.06259566), array(-448.02024561)]\n",
      "[array(-95.29392648), array(-230.65255955), array(-445.57398436)]\n",
      "[array(-92.6283119), array(-232.24828555), array(-440.3065717)]\n",
      "[array(-98.54675517), array(-230.43545728), array(-452.07096384)]\n",
      "[array(-98.24847932), array(-232.33774037), array(-451.55049804)]\n",
      "[array(-97.99397109), array(-232.32691742), array(-451.04104818)]\n",
      "[array(-95.82500258), array(-233.07840573), array(-446.7331629)]\n",
      "[array(-94.24946974), array(-228.88255645), array(-443.41427403)]\n",
      "[array(-94.35220437), array(-232.47478704), array(-443.76342034)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x_sim = sample_x(*θ_val)\n",
    "    output_sim = zmap_optimization(cost_fun_with_grad, z_val, x_sim, θ_val, random_init=False)\n",
    "    zmap_sim = mapping_fn(output_sim.x)\n",
    "    print(compile_grad_theta_fn(*zmap_sim, *x_sim, *θ_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d5120b9-54b7-497d-a241-0ed2b4513058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x_sim[0][:4], x_val[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74e52143-2e33-4984-8428-0bfa21cfac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.07251638,  0.30974791, -0.45845313,  0.29817301]),\n",
       " array([ 0.63471893,  0.48443256,  0.06441641, -0.21877776]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zmap_sim[0][:4], zmap_val[0][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20141d-9dd9-479b-93bb-3f1cea684a92",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "```python\n",
    "θ = # initial guess for θ\n",
    "H = # some guess for Hessian of θ -> logP(θ|x)\n",
    "\n",
    "while norm(θ - θlast) < θtol:\n",
    "    # a bunch of simulated x's generated from P(x,z|θ)\n",
    "    x_sims = [sample_prior_predictive(θ).x for i in 1:nsims] \n",
    "\n",
    "    # zMAP maximizes the function z -> logP(x,z|θ)\n",
    "    zMAP_data = zMAP(x, θ)\n",
    "    zMAP_sims = [zMAP(x_sim, θ) for x_sim in x_sims]\n",
    "\n",
    "    # ∇θ_logLike is gradient of θ -> logP(x,z|θ)\n",
    "    g_data = ∇θ_logLike(zMAP_data, x, θ)\n",
    "    g_sims = [∇θ_logLike(zMAP_sim, x_sim, θ) for (zMAP_sim, x_sim) in zip(zMAP_sims,x_sims)]\n",
    "\n",
    "    # gradient of θ -> logP(θ)\n",
    "    g_prior = ∇θ_logPrior(θ) \n",
    "\n",
    "    θlast = θ\n",
    "    θ -= H \\ (g_data - mean(g_sims) + g_prior)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b861e904-aef7-4d4e-8719-9e5557fad37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flatten_replace_var(values: list, name=''):\n",
    "    flatten_var = at.vector(name='flatten_' + name)\n",
    "    split_point = np.concatenate([np.asarray([0]), \n",
    "                                  np.cumsum([v.size for v in values])],\n",
    "                                 axis=-1)\n",
    "    replace_var = []\n",
    "    for i, np_val in enumerate(values):\n",
    "        replace_var.append(\n",
    "            at.reshape(flatten_var[split_point[i]:split_point[i+1]], np_val.shape))\n",
    "\n",
    "    flatten_var_value = np.concatenate([v.ravel() for v in values], axis=-1)\n",
    "    return replace_var, flatten_var, flatten_var_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86d31725-d037-4d5f-8ed5-e00306e28965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-1.10469299), array(0.26289206), array(7.74322965)]\n",
      "[array(-1.1057464), array(0.26327243), array(2.67192363)]\n",
      "[array(-1.10895806), array(0.26335883), array(1.91502821)]\n",
      "[array(-1.10352368), array(0.26458809), array(2.46085678)]\n",
      "[array(-1.10628583), array(0.26475283), array(1.93849005)]\n",
      "[array(-1.10164844), array(0.26587059), array(2.39913608)]\n",
      "[array(-1.10383368), array(0.26607752), array(1.97857484)]\n",
      "[array(-1.1003633), array(0.26707612), array(2.33860552)]\n",
      "[array(-1.1021535), array(0.26734759), array(2.01116737)]\n",
      "[array(-1.09922355), array(0.26818032), array(2.27311521)]\n",
      "[array(-1.10055724), array(0.26854053), array(2.04832752)]\n",
      "[array(-1.09831612), array(0.26930316), array(2.24781228)]\n",
      "[array(-1.09939085), array(0.26970165), array(2.06671234)]\n",
      "[array(-1.09754161), array(0.27044131), array(2.23512953)]\n",
      "[array(-1.09836194), array(0.27085112), array(2.07851889)]\n",
      "[array(-1.09683527), array(0.27154901), array(2.20860284)]\n",
      "[array(-1.09720508), array(0.27197305), array(2.09171319)]\n",
      "[array(-1.09589868), array(0.27264945), array(2.19912523)]\n",
      "[array(-1.09632469), array(0.27313536), array(2.11092325)]\n",
      "[array(-1.09583709), array(0.27378169), array(2.1571571)]\n",
      "[array(-1.09563265), array(0.27431249), array(2.13154097)]\n",
      "[array(-1.09491953), array(0.27489277), array(2.15915117)]\n",
      "[array(-1.09489259), array(0.27542414), array(2.12226183)]\n",
      "[array(-1.09420666), array(0.27601486), array(2.15158706)]\n",
      "[array(-1.09386596), array(0.27657376), array(2.14555666)]\n",
      "[array(-1.09346368), array(0.27712111), array(2.13732452)]\n",
      "[array(-1.0930133), array(0.27770227), array(2.1470214)]\n",
      "[array(-1.09270972), array(0.27826703), array(2.13995779)]\n",
      "[array(-1.09222447), array(0.2788527), array(2.15259395)]\n",
      "[array(-1.0918535), array(0.2793961), array(2.13858337)]\n",
      "[array(-1.09125635), array(0.27997892), array(2.1553026)]\n",
      "[array(-1.09075404), array(0.28052595), array(2.14973987)]\n",
      "[array(-1.09072102), array(0.28112135), array(2.1377836)]\n",
      "[array(-1.09032632), array(0.2817469), array(2.15979825)]\n",
      "[array(-1.09019484), array(0.28229819), array(2.13255383)]\n",
      "[array(-1.08970652), array(0.28292967), array(2.16110977)]\n",
      "[array(-1.08947698), array(0.28343386), array(2.11803573)]\n",
      "[array(-1.08864493), array(0.28409952), array(2.17910671)]\n",
      "[array(-1.08864778), array(0.28460674), array(2.12159098)]\n",
      "[array(-1.08789808), array(0.28525571), array(2.16955528)]\n",
      "[array(-1.08780818), array(0.28575602), array(2.11469605)]\n",
      "[array(-1.08662197), array(0.28640487), array(2.18598888)]\n",
      "[array(-1.08704728), array(0.28690161), array(2.09685285)]\n",
      "[array(-1.0857922), array(0.28761512), array(2.19517644)]\n",
      "[array(-1.08603011), array(0.28808284), array(2.10323069)]\n",
      "[array(-1.08482304), array(0.2887779), array(2.19049329)]\n",
      "[array(-1.08500181), array(0.28926345), array(2.10975918)]\n",
      "[array(-1.08408022), array(0.2899463), array(2.17490019)]\n",
      "[array(-1.08420821), array(0.29042622), array(2.0961379)]\n",
      "[array(-1.08309009), array(0.29117404), array(2.19508577)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " {'θ0': array(-1.08309009), 'θ1': array(0.29117404), 'σ': array(2.19508577)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from scipy.optimize import OptimizeWarning\n",
    "\n",
    "def MUSE(model: pm.Model, tol=1e-2, nsims=100, \n",
    "         max_iter=50,\n",
    "         minimize_method='L-BFGS-B', \n",
    "         **minimize_kwargs):\n",
    "    if model.potentials:\n",
    "        raise Exception(\"MUSE does not work with model that contains potentials\")\n",
    "        \n",
    "    # Catigorize variables into θ, z, x\n",
    "    model_graph = pm.model_graph.ModelGraph(model)\n",
    "    # θ are priors with no parent\n",
    "    theta = [var for var in model.basic_RVs if model_graph.get_parents(var) == set()]\n",
    "    # The remaining free variables are z\n",
    "    z = [var for var in model.free_RVs if var not in theta]\n",
    "    x: list = model.observed_RVs\n",
    "\n",
    "    # Compute logp and gradient\n",
    "    # For each node in the model, get the correspondent variable for computing logp\n",
    "    rv_values = {}\n",
    "    for var in z + x + theta:\n",
    "        if var in x:\n",
    "            value_var = var.type()\n",
    "            value_var.name = var.name\n",
    "        else:\n",
    "            value_var = model.rvs_to_values[var]\n",
    "        rv_values[var] = value_var\n",
    "\n",
    "    rv_logps = joint_logpt(list(rv_values.keys()), rv_values, sum=False, jacobian=True)\n",
    "    logpt_nodes = {}\n",
    "    for k, logp in zip(rv_values.keys(), rv_logps):\n",
    "        node_logp = logp.sum()\n",
    "        node_logp.name = k.name + \"_logpt\"\n",
    "        logpt_nodes[k] = node_logp\n",
    "\n",
    "    # A dict containing array so we can use to infer shape \n",
    "    test_point = model.recompute_initial_point()\n",
    "    # θ, z, x in unbounded space (variable that actually used for computing logp)\n",
    "    # and their test value\n",
    "    input_theta = [rv_values[v] for v in theta]\n",
    "    theta_val = [test_point[rv_values[v].name] for v in theta]\n",
    "    input_z = [rv_values[v] for v in z]\n",
    "    z_val = [test_point[rv_values[v].name] for v in z]\n",
    "    input_x = [rv_values[v] for v in x]\n",
    "    x_val = [model.rvs_to_values[v].data for v in x]\n",
    "    \n",
    "    # Flatten and concat θ into 1D tensors\n",
    "    (replace_theta,\n",
    "     flatten_theta,\n",
    "     init_theta,\n",
    "     ) = create_flatten_replace_var(theta_val, name='theta')\n",
    "    # Replace theta in original space\n",
    "    replace_theta_org = {}\n",
    "    for org_var, input_var, replace_var in zip(theta, input_theta, replace_theta):\n",
    "        if hasattr(input_var.tag, \"transform\"):\n",
    "            replace_theta_org[org_var] = input_var.tag.transform.backward(replace_var)\n",
    "        else:\n",
    "            replace_theta_org[org_var] = replace_var\n",
    "    # Function to sample x conditioned on θ\n",
    "    x_clone = aesara.clone_replace(x, replace_theta_org)\n",
    "    sample_x: callable = aesara.function([flatten_theta], x_clone)\n",
    "    mapping_theta_fn = aesara.function([flatten_theta], list(replace_theta_org.values()))\n",
    "    # _ = sample_x(init_theta)\n",
    "\n",
    "    # Flatten z into 1D tensor\n",
    "    (replace_z,\n",
    "     flatten_z,\n",
    "     init_z,\n",
    "     ) = create_flatten_replace_var(z_val, name='z')\n",
    "\n",
    "    # Prepare function for MAP estimate of z\n",
    "    # logP(x,z|θ)\n",
    "    condition_logpt = at.sum([logpt_nodes[var] for var in x + z])\n",
    "    condition_logpt_clone = aesara.clone_replace(\n",
    "        condition_logpt, dict(zip(input_z + input_theta,\n",
    "                                  replace_z + replace_theta)))\n",
    "    \n",
    "    # We minimize the negative loglikelihood\n",
    "    neg_condition_logpt = -1.0 * condition_logpt_clone\n",
    "    grad_z_clone_tensor = aesara.grad(\n",
    "        neg_condition_logpt,\n",
    "        wrt=flatten_z, \n",
    "        consider_constant=input_x + input_theta\n",
    "    )\n",
    "    cost_fun_with_grad = aesara.function(\n",
    "        [flatten_z] + input_x + [flatten_theta], \n",
    "        [neg_condition_logpt, grad_z_clone_tensor])\n",
    "\n",
    "    # gradient of θ -> logP(x,z|θ)\n",
    "    grad_theta_clone_tensor = aesara.grad(\n",
    "        condition_logpt_clone,\n",
    "        wrt=flatten_theta, \n",
    "        consider_constant=input_x + input_z\n",
    "    )\n",
    "    compile_grad_theta_fn = aesara.function(\n",
    "        [flatten_z] + input_x + [flatten_theta],\n",
    "        grad_theta_clone_tensor)\n",
    "    \n",
    "    # # testing\n",
    "    # grad_test = aesara.grad(condition_logpt, wrt=input_theta, consider_constant=input_x + input_z)\n",
    "    # test_fn = aesara.function(input_z + input_x + input_theta, grad_test)\n",
    "    # map_fn_z = aesara.function([flatten_z], replace_z)\n",
    "    # map_fn_θ = aesara.function([flatten_theta], replace_theta)\n",
    "    # test_z = np.random.randn(*init_z.shape)\n",
    "    # test_θ = np.random.randn(*init_theta.shape)\n",
    "    # return compile_grad_theta_fn(test_z, *x_val, test_θ), test_fn(*map_fn_z(test_z), *x_val, *map_fn_θ(test_θ))\n",
    "    \n",
    "    # gradient of θ -> logP(θ)\n",
    "    theta_logpt = at.sum([logpt_nodes[var] for var in theta])\n",
    "    theta_logpt_clone = aesara.clone_replace(\n",
    "        theta_logpt, dict(zip(input_theta, replace_theta)))\n",
    "    grad_theta_prior = aesara.grad(\n",
    "        theta_logpt_clone, \n",
    "        wrt=flatten_theta\n",
    "    )\n",
    "    compile_grad_theta_prior_fn = aesara.function([flatten_theta], grad_theta_prior)\n",
    "    \n",
    "    # testing\n",
    "    # grad_test = aesara.grad(theta_logpt, wrt=input_theta)\n",
    "    # test_fn = aesara.function(input_theta, grad_test)\n",
    "    # return compile_grad_theta_prior_fn(init_theta), test_fn(*theta_val)\n",
    "\n",
    "    # MUSE algorithm\n",
    "    theta_est = np.random.randn(*init_theta.shape)\n",
    "    last_theta = np.random.randn(*init_theta.shape)\n",
    "    H = - np.eye(len(init_theta)) * 500.\n",
    "    i = 0\n",
    "    while (np.linalg.norm(theta_est - last_theta) > tol) & (i < max_iter):\n",
    "        i += 1\n",
    "        output = minimize(\n",
    "            cost_fun_with_grad, init_z, args=(*x_val, theta_est), \n",
    "            method=minimize_method, jac=True, **minimize_kwargs\n",
    "        )\n",
    "        if not output.success:\n",
    "            warnings.warn(\"zMAP did not converge.\", OptimizeWarning)\n",
    "        zMAP_data = output.x\n",
    "        g_data = compile_grad_theta_fn(zMAP_data, *x_val, theta_est)\n",
    "        g_prior = compile_grad_theta_prior_fn(theta_est)\n",
    "        \n",
    "        g_sims = np.zeros([nsims, *g_data.shape], dtype=g_data.dtype)\n",
    "        for j in range(nsims):\n",
    "            x_sim = sample_x(theta_est)\n",
    "            output_sim = minimize(\n",
    "                cost_fun_with_grad, init_z, args=(*x_sim, theta_est), \n",
    "                method=minimize_method, jac=True, **minimize_kwargs\n",
    "            )\n",
    "            zMAP_sim = output_sim.x\n",
    "            g_sim = compile_grad_theta_fn(zMAP_sim, *x_sim, theta_est)\n",
    "            g_sims[j] = g_sim\n",
    "\n",
    "        expect_g_sim = np.mean(g_sims, axis=0)\n",
    "        last_theta = theta_est\n",
    "        theta_est = theta_est - np.linalg.solve(H, (g_data - expect_g_sim + g_prior))\n",
    "        print(mapping_theta_fn(theta_est))\n",
    "\n",
    "    return i, {\n",
    "        org_var.name: est \n",
    "        for org_var, est in zip(\n",
    "            replace_theta_org.keys(),\n",
    "            mapping_theta_fn(theta_est))\n",
    "    }\n",
    "\n",
    "MUSE(funnel, nsims=100, tol=1e-5, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc22b07-7599-42ea-a43e-4693540b3159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
