{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0827192-9773-4bf9-a334-21cb9844246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "import aesara\n",
    "import aesara.tensor as at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8535ab42-3b81-4cb9-9383-a4884f6cf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_EXAMPLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a3ad92-8319-49b7-998b-78ffe456bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(SIMPLE_EXAMPLE, x_obs=None):\n",
    "    if SIMPLE_EXAMPLE:\n",
    "        with pm.Model() as funnel:\n",
    "            θ = pm.Normal(\"θ\", 0, 3)\n",
    "            z = pm.Normal(\"z\", 0, at.exp(θ / 2), size=512)\n",
    "            x = pm.Normal(\"x\", z, 1, observed=x_obs)\n",
    "    else:\n",
    "        with pm.Model() as funnel:\n",
    "            θ0 = pm.Normal(\"θ0\", 0, 3)\n",
    "            z0 = pm.Normal(\"z0\", 0, at.exp(θ0 / 2), size=256)\n",
    "            θ1 = pm.HalfNormal(\"θ1\", 5)\n",
    "            z1 = pm.Normal(\"z1\", 0, θ1, size=(128, 2))\n",
    "            σ = pm.HalfNormal(\"σ\", 1)\n",
    "            x = pm.Normal(\"x\", at.stack([z0, at.flatten(z1)]), σ, observed=x_obs)\n",
    "    return funnel\n",
    "\n",
    "m = gen_model(SIMPLE_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b4b49-7ac1-4b8d-a9a4-366d7a757286",
   "metadata": {},
   "source": [
    "Generate data for `x` condition on some true $\\theta$. There are a few ways to do it as explained in https://github.com/pymc-devs/pymc/discussions/5280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b9d481-9686-40cf-a79e-89f553f9fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SIMPLE_EXAMPLE:\n",
    "    sample_x: callable = aesara.function([m.θ], [m.x])\n",
    "    x_obs, = sample_x(1.)\n",
    "else:\n",
    "    sample_x: callable = aesara.function([m.θ0, m.θ1, m.σ], [m.x])\n",
    "    x_obs, = sample_x(0., 1., 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6888bd47-1f2f-425c-a624-cb39f3542b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_obs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c3db7-b98e-44ab-86aa-2ddaf2d5953b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a regular PyMC model that conditioned on some observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30396006-2eef-4a07-bb68-c9afe874dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel = gen_model(SIMPLE_EXAMPLE, x_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cba126-7925-4fc4-9233-fed52a40db46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Forward sampling function (for generating `x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d66aff-ab8d-4d2a-80a9-6fd700b22b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_x_z = aesara.function([θ], [z, x])\n",
    "model_graph = pm.model_graph.ModelGraph(funnel)\n",
    "# theta are priors with no parent\n",
    "theta = [var for var in funnel.basic_RVs if model_graph.get_parents(var) == set()]\n",
    "# The remaining free variables are z\n",
    "latent_field = [var for var in funnel.free_RVs if var not in theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16668ae9-2780-4056-a2c9-60a2fd2180ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[θ0, z0, θ1, z1, σ, x]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funnel.basic_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba28c640-df3b-46e2-8380-642c9909c50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([θ0, θ1, σ], [z0, z1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta, latent_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522d51a7-fb65-42da-bc2f-160baa6aa89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 256), (256,), (128, 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z_x = list(set(funnel.value_vars) - set(theta))  # Not doing this as the order is unpredictable\n",
    "z_x: list = funnel.observed_RVs + latent_field\n",
    "\n",
    "sample_x_z: callable = aesara.function(theta, z_x)\n",
    "theta_val = [v.eval() for v in theta]\n",
    "output_test = sample_x_z(*theta_val)\n",
    "[v.shape for v in output_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f27cbd-4b01-4aab-a710-d0a0d1afa578",
   "metadata": {},
   "source": [
    "Alternative `sample_x` that only output the simulation of the observed (`x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9015bf32-0266-48c1-a8fe-defd7f53a61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 256)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x: callable = aesara.function(theta, funnel.observed_RVs)\n",
    "theta_val = [v.eval() for v in theta]\n",
    "output_test = sample_x(*theta_val)\n",
    "[v.shape for v in output_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317540b-37e9-494c-90d1-2d019a23b2a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Likelihood function `logP(x,z|θ)` and Prior function `logP(θ)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3ba9032-9b44-4a5d-b8ee-53a636af06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc.distributions import logpt as joint_logpt\n",
    "\n",
    "# Copy and small modification of self.logp_elemwiset in a pm.Model\n",
    "def generate_logpt_allnodes(model, vars: list, jacobian: bool = True):\n",
    "    \"\"\"Elemwise log-probability of the input variables.\"\"\"\n",
    "    if model.potentials:\n",
    "        raise Exception(\"Does not work with model that contains potentials\")\n",
    "\n",
    "    rv_values = {}\n",
    "    for var in vars:\n",
    "        if var in model.observed_RVs:\n",
    "            value_var = var.type()\n",
    "            value_var.name = var.name\n",
    "        else:\n",
    "            value_var = model.rvs_to_values[var]\n",
    "        if value_var is not None:\n",
    "            rv_values[var] = value_var\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Requested variable {var} not found among the model variables\"\n",
    "            )\n",
    "\n",
    "    rv_logps = joint_logpt(list(rv_values.keys()), rv_values, sum=False, jacobian=jacobian)\n",
    "    logpt_nodes = {}\n",
    "    for k, logp in zip(rv_values.keys(), rv_logps):\n",
    "        node_logp = logp.sum()\n",
    "        node_logp.name = k.name + \"_logpt\"\n",
    "        logpt_nodes[k] = node_logp\n",
    "\n",
    "    return logpt_nodes, rv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b668e5c-e3c3-49c1-9682-0bec34a3ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the order of z, x, θ\n",
    "ordered_input_var: list = latent_field + funnel.observed_RVs + theta\n",
    "\n",
    "logpt_nodes, rv_values = generate_logpt_allnodes(funnel, ordered_input_var)\n",
    "input_var = [rv_values[v] for v in ordered_input_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4cf0351-24bb-4536-b142-4572fe79bd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{z0: z0_logpt,\n",
       " z1: z1_logpt,\n",
       " x: x_logpt,\n",
       " θ0: θ0_logpt,\n",
       " θ1: θ1_logpt,\n",
       " σ: σ_logpt}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logpt_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c7bbe8a-995f-4dd1-ba30-c01d0fba772b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{z0: z0, z1: z1, x: x, θ0: θ0, θ1: θ1_log__, σ: σ_log__}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1536bf2f-1014-46df-965d-bb4d86fcf124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-235.2482645), array(-647.26437008), array(-1749.97194133), array(-2.01755082), array(-0.72579135), array(-0.72579135)]\n",
      "[z0, z1, x, θ0, θ1, σ]\n",
      "\n",
      "From PyMC model itself\n",
      "θ0      -2.02\n",
      "z0    -235.25\n",
      "θ1      -0.73\n",
      "z1    -647.26\n",
      "σ       -0.73\n",
      "x    -1749.97\n",
      "Name: Point log-probability, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Some testing\n",
    "compile_logp_fn_per_node = aesara.function(\n",
    "    input_var, [logpt_nodes[v] for v in ordered_input_var])\n",
    "test_point = funnel.recompute_initial_point()\n",
    "\n",
    "z_val = [test_point[rv_values[v].name] for v in latent_field]\n",
    "θ_val = [test_point[rv_values[v].name] for v in theta]\n",
    "\n",
    "x_val = [funnel.rvs_to_values[v].data for v in funnel.observed_RVs]\n",
    "\n",
    "print(compile_logp_fn_per_node(*z_val, *x_val, *θ_val))\n",
    "print(ordered_input_var)\n",
    "print(\"\\nFrom PyMC model itself\")\n",
    "print(funnel.point_logps(test_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8669dade-0184-4edd-b9df-7cd8237e4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpt_z_x = []\n",
    "logpt_theta = []\n",
    "input_theta = []\n",
    "for var in ordered_input_var:\n",
    "    if var not in theta:\n",
    "        logpt_z_x.append(logpt_nodes[var])\n",
    "    else:\n",
    "        logpt_theta.append(logpt_nodes[var])\n",
    "        input_theta.append(rv_values[var])\n",
    "\n",
    "condition_logpt = at.sum(logpt_z_x)\n",
    "compile_logp_fn = aesara.function(input_var, [condition_logpt])\n",
    "theta_logpt = at.sum(logpt_theta)\n",
    "compile_logp_fn_theta = aesara.function(input_theta, [theta_logpt])\n",
    "\n",
    "# compare to full posterior as check\n",
    "np.testing.assert_almost_equal(sum(compile_logp_fn(*z_val, *x_val, *θ_val) + compile_logp_fn_theta(*θ_val)),\n",
    "                               funnel.logp(test_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ddf65c-53d3-4b38-b3ca-30c35d4d47a6",
   "metadata": {},
   "source": [
    "## ∇θ_logLike and ∇θ_logPrior(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69470930-b7ca-4aa4-9be5-a85e2aeaa381",
   "metadata": {},
   "source": [
    "∇θ_logLike is gradient of θ -> logP(x,z|θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dab433e-086f-4db0-a98b-13776bb7adbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(-128.), array(-256.), array(2046.95082465)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_theta_logpt = aesara.grad(\n",
    "    condition_logpt, \n",
    "    wrt=input_theta, \n",
    "    consider_constant=list(set(input_var) - set(input_theta))\n",
    ")\n",
    "compile_grad_theta_fn = aesara.function(input_var, grad_theta_logpt)\n",
    "compile_grad_theta_fn(*z_val, *x_val, *θ_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac4d76-cee5-4d67-8a74-571b8f2879c2",
   "metadata": {},
   "source": [
    "∇θ_logPrior(θ) is gradient of θ -> logPrior(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c02af32b-0b13-467e-a5a9-80614d57058d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(-0.), array(1.11022302e-16), array(0.)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_theta_prior = aesara.grad(\n",
    "    theta_logpt, \n",
    "    wrt=input_theta\n",
    ")\n",
    "compile_grad_theta_fn = aesara.function(input_theta, grad_theta_prior)\n",
    "compile_grad_theta_fn(*θ_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92b965-b968-4140-b553-66bf0fbe1120",
   "metadata": {
    "tags": []
   },
   "source": [
    "## zMAP that maximizes the function z -> logP(x,z|θ)\n",
    "\n",
    "We can use [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) to find the MAP.\n",
    "While we have logP(x,z|θ) and gradient of z -> logP(x,z|θ) above, to make using scipy minimize easier we to some additional formatting so that:\n",
    "- `cost_fun` to return a tuple (f, g) containing the objective function and the gradient\n",
    "- Flatten `z` to an 1-D array with shape (n,)\n",
    "\n",
    "Similar logic and API useage see [find_MAP in PyMC](https://github.com/pymc-devs/pymc/blob/1d7130d8cf6e419120e192f8308cf154f4c44074/pymc/tuning/starting.py#L148-L150) and [ValueGradFunction](https://github.com/pymc-devs/pymc/blob/5626a04a1e064ad615e1765e37bcb7ea52887ab7/pymc/model.py#L358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a4e2255-c626-4aec-93fc-596b729df57f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ∇z_logLike is gradient of z -> logP(x,z|θ) that will be used in optimization\n",
    "input_z = []\n",
    "input_x_theta = []\n",
    "for var in ordered_input_var:\n",
    "    if var in latent_field:\n",
    "        input_z.append(rv_values[var])\n",
    "    else:\n",
    "        input_x_theta.append(rv_values[var])\n",
    "\n",
    "# Flatten and replace value (similar to ValueGradFunction in pm.Model)\n",
    "flatten_z = at.vector(name='flatten_z')\n",
    "split_point = np.concatenate([np.asarray([0]), np.cumsum([v.size for v in z_val])], axis=-1)\n",
    "z_replace = []\n",
    "for i, np_val in enumerate(z_val):\n",
    "    z_replace.append(at.reshape(flatten_z[split_point[i]:split_point[i+1]], np_val.shape))\n",
    "\n",
    "mapping_fn = aesara.function([flatten_z], z_replace)\n",
    "flatten_z_val = np.concatenate([v.ravel() for v in z_val], axis=-1)\n",
    "# mapping_fn(flatten_z_val)\n",
    "\n",
    "# We minimize the negative loglikelihood\n",
    "condition_logpt_clone = -1.0 * aesara.clone_replace(condition_logpt, dict(zip(input_z, z_replace)))\n",
    "grad_z_clone_tensor = aesara.grad(\n",
    "    condition_logpt_clone,\n",
    "    wrt=flatten_z, \n",
    "    consider_constant=input_x_theta\n",
    ")\n",
    "cost_fun_with_grad = aesara.function(\n",
    "    [flatten_z] + input_x_theta, \n",
    "    [condition_logpt_clone, grad_z_clone_tensor])\n",
    "value_test, grad_test = cost_fun_with_grad(flatten_z_val, *x_val, *θ_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad92c7e9-27fd-493c-aa0c-a63014d213df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some testing\n",
    "grad_z_tensor = aesara.grad(\n",
    "    -condition_logpt,\n",
    "    wrt=input_z, \n",
    "    consider_constant=input_x_theta\n",
    ")\n",
    "\n",
    "output_tensors = [-condition_logpt] + grad_z_tensor\n",
    "cost_fun_with_grad_ = aesara.function(input_var, output_tensors)\n",
    "value_test2, *grad_test2 = cost_fun_with_grad_(*z_val, *x_val, *θ_val)\n",
    "\n",
    "assert value_test == value_test2\n",
    "_ = [np.testing.assert_almost_equal(v1, v2) for v1, v2 in zip(mapping_fn(grad_test), grad_test2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d76a0-b0df-45d2-a4d5-5e83fb95419f",
   "metadata": {},
   "source": [
    "### Using PyMC idioms\n",
    "\n",
    "But either have the exact input -> output signature we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45af9f0b-f152-49f7-b47a-896eeebcdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point['x'] = x_val[0]\n",
    "# shared = pm.make_shared_replacements(test_point, input_z, funnel)\n",
    "shared = {\n",
    "    var: aesara.shared(test_point[var.name],\n",
    "                       var.name + \"_shared\",\n",
    "                       broadcastable=var.broadcastable)\n",
    "    for var in input_x_theta\n",
    "}\n",
    "out_list, inarray0 = pm.join_nonshared_inputs(\n",
    "    test_point, [-condition_logpt] + grad_z_tensor, input_var, shared)\n",
    "cost_fun_with_grad_ = pm.aesaraf.compile_pymc([inarray0], out_list)\n",
    "cost_fun_with_grad_.trust_input = True\n",
    "value_test3, *grad_test3 = cost_fun_with_grad_(flatten_z_val)\n",
    "\n",
    "assert value_test2 == value_test3\n",
    "_ = [np.testing.assert_almost_equal(v1, v2) for v1, v2 in zip(grad_test2, grad_test3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "129fc895-ca95-4669-a023-a6bc7e2cb7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_vars_and_values = {\n",
    "    var: test_point[var.name]\n",
    "    for var in input_x_theta\n",
    "}\n",
    "pm_val_grad_fn = pm.model.ValueGradFunction([-condition_logpt], input_z, extra_vars_and_values)\n",
    "pm_val_grad_fn.set_extra_values(test_point)\n",
    "value_test4, grad_test4 = pm_val_grad_fn(z_val)\n",
    "\n",
    "assert value_test == value_test3\n",
    "np.testing.assert_almost_equal(grad_test, grad_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915b9ae-1acb-4e86-bb13-407119eeeea3",
   "metadata": {},
   "source": [
    "Once we have the logp fn that also output gradient, optimizing it is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7852857-3b66-44fa-9968-86e287c57afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def zmap_optimization(\n",
    "    cost_fun_with_grad: callable,\n",
    "    initial_z: list,\n",
    "    x: list,\n",
    "    theta: list,\n",
    "    random_init=True,\n",
    "    method='L-BFGS-B',\n",
    "    **kwargs):\n",
    "    x0 = np.concatenate([v.ravel() for v in initial_z], axis=-1)\n",
    "    if random_init:\n",
    "        x0 = np.random.randn(*x0.shape)\n",
    "    return minimize(\n",
    "        cost_fun_with_grad, x0, args=(*x, *theta), method=method, jac=True, **kwargs\n",
    "    )\n",
    "\n",
    "output = zmap_optimization(cost_fun_with_grad, z_val, x_val, θ_val, random_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6ade842-0a17-4013-a878-f596320533ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "zmap_val = mapping_fn(output.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "198b60b2-1151-4e82-b463-4359487c2eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(18.04109433), array(-204.57177619), array(-217.85975446)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compile_grad_theta_fn = aesara.function(input_var, grad_theta_logpt)\n",
    "compile_grad_theta_fn(*zmap_val, *x_val, *θ_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c3fdc3b-8036-4c22-b9ea-5e74ec64a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-95.54823446), array(-226.85136644), array(-445.93056232)]\n",
      "[array(-92.50569887), array(-233.21234482), array(-440.09990346)]\n",
      "[array(-93.97201758), array(-234.74582283), array(-443.09387708)]\n",
      "[array(-90.81994555), array(-232.88403009), array(-436.71526112)]\n",
      "[array(-94.10079006), array(-231.75049754), array(-443.23162237)]\n",
      "[array(-94.53426237), array(-230.4842632), array(-444.04792321)]\n",
      "[array(-91.7516157), array(-233.21778535), array(-438.59195264)]\n",
      "[array(-94.31463062), array(-229.69095847), array(-443.5769296)]\n",
      "[array(-90.92862597), array(-232.6731741), array(-436.92418894)]\n",
      "[array(-99.24366179), array(-234.5230246), array(-453.62826915)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    x_sim = sample_x(*θ_val)\n",
    "    output_sim = zmap_optimization(cost_fun_with_grad, z_val, x_sim, θ_val, random_init=False)\n",
    "    zmap_sim = mapping_fn(output_sim.x)\n",
    "    print(compile_grad_theta_fn(*zmap_sim, *x_sim, *θ_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d5120b9-54b7-497d-a241-0ed2b4513058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x_sim[0][:4], x_val[0][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74e52143-2e33-4984-8428-0bfa21cfac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.54025815,  0.61958206, -0.08493585, -0.77881476]),\n",
       " array([-0.10804626, -0.72991171, -0.73551543,  0.42465603]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zmap_sim[0][:4], zmap_val[0][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20141d-9dd9-479b-93bb-3f1cea684a92",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "```python\n",
    "θ = # initial guess for θ\n",
    "H = # some guess for Hessian of θ -> logP(θ|x)\n",
    "\n",
    "while norm(θ - θlast) < θtol:\n",
    "    # a bunch of simulated x's generated from P(x,z|θ)\n",
    "    x_sims = [sample_prior_predictive(θ).x for i in 1:nsims] \n",
    "\n",
    "    # zMAP maximizes the function z -> logP(x,z|θ)\n",
    "    zMAP_data = zMAP(x, θ)\n",
    "    zMAP_sims = [zMAP(x_sim, θ) for x_sim in x_sims]\n",
    "\n",
    "    # ∇θ_logLike is gradient of θ -> logP(x,z|θ)\n",
    "    g_data = ∇θ_logLike(zMAP_data, x, θ)\n",
    "    g_sims = [∇θ_logLike(zMAP_sim, x_sim, θ) for (zMAP_sim, x_sim) in zip(zMAP_sims,x_sims)]\n",
    "\n",
    "    # gradient of θ -> logP(θ)\n",
    "    g_prior = ∇θ_logPrior(θ) \n",
    "\n",
    "    θlast = θ\n",
    "    θ -= H \\ (g_data - mean(g_sims) + g_prior)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b861e904-aef7-4d4e-8719-9e5557fad37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flatten_replace_var(values: list, name=''):\n",
    "    flatten_var = at.vector(name='flatten_' + name)\n",
    "    split_point = np.concatenate([np.asarray([0]), \n",
    "                                  np.cumsum([v.size for v in values])],\n",
    "                                 axis=-1)\n",
    "    replace_var = []\n",
    "    for i, np_val in enumerate(values):\n",
    "        replace_var.append(\n",
    "            at.reshape(flatten_var[split_point[i]:split_point[i+1]], np_val.shape))\n",
    "\n",
    "    flatten_var_value = np.concatenate([v.ravel() for v in values], axis=-1)\n",
    "    return replace_var, flatten_var, flatten_var_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86d31725-d037-4d5f-8ed5-e00306e28965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-0.87957671), array(2.92824651), array(1.39199477)]\n",
      "[array(-0.85819613), array(2.65471532), array(1.65906021)]\n",
      "[array(-0.85055661), array(2.44077123), array(1.76714863)]\n",
      "[array(-0.84602453), array(2.28631278), array(1.8174909)]\n",
      "[array(-0.84266008), array(2.16390124), array(1.84085601)]\n",
      "[array(-0.83971972), array(2.06617234), array(1.85637519)]\n",
      "[array(-0.83693359), array(1.98303401), array(1.86887439)]\n",
      "[array(-0.83450394), array(1.91211838), array(1.87281724)]\n",
      "[array(-0.83201881), array(1.85405146), array(1.8836189)]\n",
      "[array(-0.82997574), array(1.80064127), array(1.88023002)]\n",
      "[array(-0.82775395), array(1.76187634), array(1.89403626)]\n",
      "[array(-0.82580339), array(1.72432229), array(1.89785518)]\n",
      "[array(-0.82395766), array(1.6923904), array(1.90268489)]\n",
      "[array(-0.82207812), array(1.66228772), array(1.90856518)]\n",
      "[array(-0.82035931), array(1.63646326), array(1.91337922)]\n",
      "[array(-0.81905189), array(1.61284208), array(1.90663344)]\n",
      "[array(-0.81729682), array(1.59422736), array(1.92066524)]\n",
      "[array(-0.81578349), array(1.57402378), array(1.9230848)]\n",
      "[array(-0.81420012), array(1.55670867), array(1.93157924)]\n",
      "[array(-0.81299153), array(1.54107609), array(1.92998154)]\n",
      "[array(-0.81154403), array(1.52582138), array(1.93589134)]\n",
      "[array(-0.81024673), array(1.51407158), array(1.94311749)]\n",
      "[array(-0.80893109), array(1.49908131), array(1.9434206)]\n",
      "[array(-0.8076924), array(1.48945049), array(1.95189719)]\n",
      "[array(-0.80673469), array(1.47599121), array(1.94191497)]\n",
      "[array(-0.80542801), array(1.46508002), array(1.9484798)]\n",
      "[array(-0.80440786), array(1.45487162), array(1.94659861)]\n",
      "[array(-0.80296049), array(1.44541072), array(1.96007672)]\n",
      "[array(-0.8020757), array(1.43673644), array(1.95625932)]\n",
      "[array(-0.80113993), array(1.42861304), array(1.95511134)]\n",
      "[array(-0.79985845), array(1.4198177), array(1.96334829)]\n",
      "[array(-0.79891826), array(1.41326764), array(1.96560785)]\n",
      "[array(-0.7981763), array(1.40559154), array(1.95784408)]\n",
      "[array(-0.79730299), array(1.3986024), array(1.95602686)]\n",
      "[array(-0.79640185), array(1.39297673), array(1.95854798)]\n",
      "[array(-0.79535997), array(1.38626285), array(1.96248605)]\n",
      "[array(-0.79446949), array(1.38173334), array(1.96725327)]\n",
      "[array(-0.79354283), array(1.37484182), array(1.96623906)]\n",
      "[array(-0.79273764), array(1.36896192), array(1.96375519)]\n",
      "[array(-0.79162535), array(1.36532555), array(1.9779847)]\n",
      "[array(-0.7908944), array(1.35799668), array(1.96802566)]\n",
      "[array(-0.79011544), array(1.35569357), array(1.97477189)]\n",
      "[array(-0.78918157), array(1.35178584), array(1.98179653)]\n",
      "[array(-0.78854946), array(1.34663962), array(1.97447057)]\n",
      "[array(-0.78805029), array(1.34146837), array(1.96245456)]\n",
      "[array(-0.78711724), array(1.33801854), array(1.97026599)]\n",
      "[array(-0.78638191), array(1.33558147), array(1.97461002)]\n",
      "[array(-0.78561614), array(1.33258549), array(1.9781196)]\n",
      "[array(-0.78501904), array(1.32822942), array(1.97135998)]\n",
      "[array(-0.78434992), array(1.32487006), array(1.97018346)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50,\n",
       " {'θ0': array(-0.78434992), 'θ1': array(1.32487006), 'σ': array(1.97018346)})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from scipy.optimize import OptimizeWarning\n",
    "\n",
    "def MUSE(model: pm.Model, tol=1e-2, nsims=100, \n",
    "         max_iter=50,\n",
    "         minimize_method='L-BFGS-B', \n",
    "         **minimize_kwargs):\n",
    "    if model.potentials:\n",
    "        raise Exception(\"MUSE does not work with model that contains potentials\")\n",
    "        \n",
    "    # Catigorize variables into θ, z, x\n",
    "    model_graph = pm.model_graph.ModelGraph(model)\n",
    "    # θ are priors with no parent\n",
    "    theta = [var for var in model.basic_RVs if model_graph.get_parents(var) == set()]\n",
    "    # The remaining free variables are z\n",
    "    z = [var for var in model.free_RVs if var not in theta]\n",
    "    x: list = model.observed_RVs\n",
    "\n",
    "    # Compute logp and gradient\n",
    "    # For each node in the model, get the correspondent variable for computing logp\n",
    "    rv_values = {}\n",
    "    for var in z + x + theta:\n",
    "        if var in x:\n",
    "            value_var = var.type()\n",
    "            value_var.name = var.name\n",
    "        else:\n",
    "            value_var = model.rvs_to_values[var]\n",
    "        rv_values[var] = value_var\n",
    "\n",
    "    rv_logps = joint_logpt(list(rv_values.keys()), rv_values, sum=False, jacobian=True)\n",
    "    logpt_nodes = {}\n",
    "    for k, logp in zip(rv_values.keys(), rv_logps):\n",
    "        node_logp = logp.sum()\n",
    "        node_logp.name = k.name + \"_logpt\"\n",
    "        logpt_nodes[k] = node_logp\n",
    "\n",
    "    # A dict containing array so we can use to infer shape \n",
    "    test_point = model.recompute_initial_point()\n",
    "    # θ, z, x in unbounded space (variable that actually used for computing logp)\n",
    "    # and their test value\n",
    "    input_theta = [rv_values[v] for v in theta]\n",
    "    theta_val = [test_point[rv_values[v].name] for v in theta]\n",
    "    input_z = [rv_values[v] for v in z]\n",
    "    z_val = [test_point[rv_values[v].name] for v in z]\n",
    "    input_x = [rv_values[v] for v in x]\n",
    "    x_val = [model.rvs_to_values[v].data for v in x]\n",
    "    \n",
    "    # Flatten and concat θ into 1D tensors\n",
    "    (replace_theta,\n",
    "     flatten_theta,\n",
    "     init_theta,\n",
    "     ) = create_flatten_replace_var(theta_val, name='theta')\n",
    "    # Replace theta in original space\n",
    "    replace_theta_org = {}\n",
    "    for org_var, input_var, replace_var in zip(theta, input_theta, replace_theta):\n",
    "        if hasattr(input_var.tag, \"transform\"):\n",
    "            replace_theta_org[org_var] = input_var.tag.transform.backward(replace_var)\n",
    "        else:\n",
    "            replace_theta_org[org_var] = replace_var\n",
    "    # Function to sample x conditioned on θ\n",
    "    x_clone = aesara.clone_replace(x, replace_theta_org)\n",
    "    sample_x: callable = aesara.function([flatten_theta], x_clone)\n",
    "    mapping_theta_fn = aesara.function([flatten_theta], list(replace_theta_org.values()))\n",
    "    # _ = sample_x(init_theta)\n",
    "\n",
    "    # Flatten z into 1D tensor\n",
    "    (replace_z,\n",
    "     flatten_z,\n",
    "     init_z,\n",
    "     ) = create_flatten_replace_var(z_val, name='z')\n",
    "\n",
    "    # Prepare function for MAP estimate of z\n",
    "    # logP(x,z|θ)\n",
    "    condition_logpt = at.sum([logpt_nodes[var] for var in x + z])\n",
    "    condition_logpt_clone = aesara.clone_replace(\n",
    "        condition_logpt, dict(zip(input_z + input_theta,\n",
    "                                  replace_z + replace_theta)))\n",
    "    \n",
    "    # We minimize the negative loglikelihood\n",
    "    neg_condition_logpt = -1.0 * condition_logpt_clone\n",
    "    grad_z_clone_tensor = aesara.grad(\n",
    "        neg_condition_logpt,\n",
    "        wrt=flatten_z, \n",
    "        consider_constant=input_x + input_theta\n",
    "    )\n",
    "    cost_fun_with_grad = aesara.function(\n",
    "        [flatten_z] + input_x + [flatten_theta], \n",
    "        [neg_condition_logpt, grad_z_clone_tensor])\n",
    "\n",
    "    # gradient of θ -> logP(x,z|θ)\n",
    "    grad_theta_clone_tensor = aesara.grad(\n",
    "        condition_logpt_clone,\n",
    "        wrt=flatten_theta, \n",
    "        consider_constant=input_x + input_z\n",
    "    )\n",
    "    compile_grad_theta_fn = aesara.function(\n",
    "        [flatten_z] + input_x + [flatten_theta],\n",
    "        grad_theta_clone_tensor)\n",
    "    \n",
    "    # # testing\n",
    "    # grad_test = aesara.grad(condition_logpt, wrt=input_theta, consider_constant=input_x + input_z)\n",
    "    # test_fn = aesara.function(input_z + input_x + input_theta, grad_test)\n",
    "    # map_fn_z = aesara.function([flatten_z], replace_z)\n",
    "    # map_fn_θ = aesara.function([flatten_theta], replace_theta)\n",
    "    # test_z = np.random.randn(*init_z.shape)\n",
    "    # test_θ = np.random.randn(*init_theta.shape)\n",
    "    # return compile_grad_theta_fn(test_z, *x_val, test_θ), test_fn(*map_fn_z(test_z), *x_val, *map_fn_θ(test_θ))\n",
    "    \n",
    "    # gradient of θ -> logP(θ)\n",
    "    theta_logpt = at.sum([logpt_nodes[var] for var in theta])\n",
    "    theta_logpt_clone = aesara.clone_replace(\n",
    "        theta_logpt, dict(zip(input_theta, replace_theta)))\n",
    "    grad_theta_prior = aesara.grad(\n",
    "        theta_logpt_clone, \n",
    "        wrt=flatten_theta\n",
    "    )\n",
    "    compile_grad_theta_prior_fn = aesara.function([flatten_theta], grad_theta_prior)\n",
    "    \n",
    "    # testing\n",
    "    # grad_test = aesara.grad(theta_logpt, wrt=input_theta)\n",
    "    # test_fn = aesara.function(input_theta, grad_test)\n",
    "    # return compile_grad_theta_prior_fn(init_theta), test_fn(*theta_val)\n",
    "\n",
    "    # MUSE algorithm\n",
    "    theta_est = np.random.randn(*init_theta.shape)\n",
    "    last_theta = np.random.randn(*init_theta.shape)\n",
    "    H = - np.eye(len(init_theta)) * 1000.\n",
    "    i = 0\n",
    "    while (np.linalg.norm(theta_est - last_theta) > tol) & (i < max_iter):\n",
    "        i += 1\n",
    "        output = minimize(\n",
    "            cost_fun_with_grad, init_z, args=(*x_val, theta_est), \n",
    "            method=minimize_method, jac=True, **minimize_kwargs\n",
    "        )\n",
    "        if not output.success:\n",
    "            warnings.warn(\"zMAP did not converge.\", OptimizeWarning)\n",
    "        zMAP_data = output.x\n",
    "        g_data = compile_grad_theta_fn(zMAP_data, *x_val, theta_est)\n",
    "        g_prior = compile_grad_theta_prior_fn(theta_est)\n",
    "        \n",
    "        g_sims = np.zeros([nsims, *g_data.shape], dtype=g_data.dtype)\n",
    "        for j in range(nsims):\n",
    "            x_sim = sample_x(theta_est)\n",
    "            output_sim = minimize(\n",
    "                cost_fun_with_grad, init_z, args=(*x_sim, theta_est), \n",
    "                method=minimize_method, jac=True, **minimize_kwargs\n",
    "            )\n",
    "            zMAP_sim = output_sim.x\n",
    "            g_sim = compile_grad_theta_fn(zMAP_sim, *x_sim, theta_est)\n",
    "            g_sims[j] = g_sim\n",
    "\n",
    "        expect_g_sim = np.mean(g_sims, axis=0)\n",
    "        last_theta = theta_est\n",
    "        theta_est = theta_est - np.linalg.solve(H, (g_data - expect_g_sim + g_prior))\n",
    "        print(mapping_theta_fn(theta_est))\n",
    "\n",
    "    return i, {\n",
    "        org_var.name: est \n",
    "        for org_var, est in zip(\n",
    "            replace_theta_org.keys(),\n",
    "            mapping_theta_fn(theta_est))\n",
    "    }\n",
    "\n",
    "MUSE(funnel, nsims=100, tol=1e-5, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc22b07-7599-42ea-a43e-4693540b3159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
