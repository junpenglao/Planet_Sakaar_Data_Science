{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion here: https://discourse.pymc.io/t/compounds-steps/954\n",
    "\n",
    "In brief, when Compound steps are involved, it takes a list of `step` to generate a list of `methods`. So for example if you do\n",
    "```python\n",
    "with pm.Model() as m:\n",
    "    rv1 = ...\n",
    "    ...\n",
    "    step1 = pm.Metropolis([rv1, rv2])\n",
    "    step2 = pm.CategoricalGibbsMetropolis([rv3])\n",
    "    trace = pm.sample(..., step=[step1, step2]...)\n",
    "```\n",
    "The Compound step is now contain a list of `methods`:\n",
    "https://github.com/pymc-devs/pymc3/blob/999661c092310b1f247f14037f795a852425e9c9/pymc3/step_methods/compound.py#L22-L32\n",
    "\n",
    "And at each sample, it iterates each method, which takes a `point` as input, and generates a new `point` as output. The new `point` is proposed within each step via a stochastic kernel, and if the proposal was rejected by MH criteria it just outputs the original input `point`\n",
    "\n",
    "Take a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">BinaryGibbsMetropolis: [ni]\n",
      ">NUTS: [p]\n",
      "100%|██████████| 1000/1000 [00:01<00:00, 517.08it/s]\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '6262' (I am process '6263')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/laoj/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '6262' (I am process '6264')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/laoj/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '6263' (I am process '6264')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/laoj/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-3.5.2-64/lock_dir\n",
      "The acceptance probability does not match the target. It is 0.8797936105241355, but should be close to 0.8. Try to increase the number of tuning steps.\n"
     ]
    }
   ],
   "source": [
    "n_=theano.shared(np.asarray([10, 15]))\n",
    "with pm.Model() as m:\n",
    "    p = pm.Beta('p', 1., 1.)\n",
    "    ni = pm.Bernoulli('ni', .5)\n",
    "    k = pm.Binomial('k', p=p, n=n_[ni], observed=4)\n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two free parameters in the model we would like to sample from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[p_logodds__, ni]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.free_RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `pm.sample()`, `PyMC3` assigns the best step method to each of them. For example, NUTS was assigned to `p_logodds__` and BinaryGibbsMetropolis was assigned to `ni`.\n",
    "\n",
    "But we can also specify the step manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    step1 = pm.Metropolis([p])\n",
    "    step2 = pm.BinaryGibbsMetropolis([ni])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can pass a point to the step, and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ni': array(0), 'p_logodds__': array(0.)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = m.test_point\n",
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ni': array(0), 'p_logodds__': array(-0.53833181)},\n",
       " [{'accept': 0.9463168972423475, 'tune': True}])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point, state = step1.step(point=point)\n",
    "point, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see, the value of  `ni` does not change, but `p_logodds__` is updated.\n",
    "\n",
    "And similarly, you can get a sample using the `step2`:\n",
    "\n",
    "(notice that there is no `generates_stats`, so only output the point here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ni': array(0), 'p_logodds__': array(-0.53833181)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = step2.step(point=point)\n",
    "point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compound step works exactly like this by iterating all the steps within the list. In effect, it is a metropolis hastings within gibbs sampling. \n",
    "\n",
    "Moreover, `pm.CompoundStep` is called internally by `pm.sample()`. We can make them explicit as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pymc3.step_methods.metropolis.Metropolis at 0x7fd94c4df828>,\n",
       " <pymc3.step_methods.metropolis.BinaryGibbsMetropolis at 0x7fd938c3bfd0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with m:\n",
    "    comp_step1 = pm.CompoundStep([step1, step2])\n",
    "comp_step1.methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes that, when in the default setting, the parameter update order follows the same order of the RVs, and it is assigned automatically. But if you specify the step you can change that order as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pymc3.step_methods.metropolis.BinaryGibbsMetropolis at 0x7fd938c3bfd0>,\n",
       " <pymc3.step_methods.metropolis.Metropolis at 0x7fd94c4df828>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with m:\n",
    "    comp_step2 = pm.CompoundStep([step2, step1])\n",
    "comp_step2.methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sampling, it always follows the same order per sample in the Gibbs like update. Notice that it is not exactly Gibbs sampling as it does not generate from a conditional probability. More precisely it updates in a Gibbs like fashion where the accept-reject is based on comparing the ratio of the conditional logp with $p \\sim \\text{Uniform}(0., 1.)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent issue/concern is the validately of mixing discrete and continuous sampling, especially mixing other sampler with NUTS. While in BDA 3rd edition Chapter 12.4 there is a small paragraph on this \"Combining HMC with Gibbs sampling\", which suggesting this could be a valid way to do, the Stan devs always skeptical about how practical it is ([1](http://discourse.mc-stan.org/t/mcmc-sampling-does-not-work-when-execute/1918/47), [2](http://discourse.mc-stan.org/t/constraining-latent-factor-model-baysian-probabalisic-matrix-factorization-to-remove-multimodality/2152/21)). \n",
    "\n",
    "\n",
    "The concern with mixing discrete and continuous sampling is that the change in discrete parameters will affect the continuous distribution's geometry so that the adaptation (i.e., the tuned mass matrix and step size) may be inappropriate. HMC/NUTS is hypersensitive to its tuning parameters (mass matrix and step size). We also don't know how many iterations we have to run to get a decent sample when the discrete parameters change. We haven't evaluated any of this, in my experience if the discrete parameter is low dimension (e.g., 2 class mixture equivalent, outliner detection with explicit discrete labelling) it works OK. However, it is much less efficient than marginalize out the discrete parameters, and you do see the chain being stuck quite often.\n",
    "It would be good to evaluate this more properly, eg to simulate data and look at posterior coverage, as explained in [Cook, Gelman, and Rubin 2006](https://amstat.tandfonline.com/doi/abs/10.1198/106186006x136976)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
